---
title: What are Embedders?
sidebarTitle: Overview
description: Learn how to use embedders with Agno to convert complex information into vector representations.
---

An Embedder converts complex information into vector representations, allowing it to be stored in a vector database. By transforming data into embeddings, the embedder enables efficient searching and retrieval of contextually relevant information. This process enhances the responses of language models by providing them with the necessary business context, ensuring they are context-aware. Agno uses the `OpenAIEmbedder` as the default embedder, but other embedders are supported as well. Here is an example:

```python
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector
from agno.knowledge.embedder.openai import OpenAIEmbedder

# Create knowledge
knowledge = Knowledge(
    vector_db=PgVector(
        db_url=db_url,
        table_name=embeddings_table,
        embedder=OpenAIEmbedder(),
    ),
    # 2 references are added to the prompt
    max_results=2,
)

# Add content to knowledge
knowledge.add_content(
    text_content="The sky is blue"
)

# Add the knowledge to the Agent
agent = Agent(knowledge=knowledge)
```

The following embedders are supported:

- [OpenAI](/concepts/knowledge/embedder/openai)
- [Cohere](/concepts/knowledge/embedder/cohere)
- [Gemini](/concepts/knowledge/embedder/gemini)
- [AWS Bedrock](/concepts/knowledge/embedder/aws_bedrock)
- [Azure OpenAI](/concepts/knowledge/embedder/azure_openai)
- [Fireworks](/concepts/knowledge/embedder/fireworks)
- [HuggingFace](/concepts/knowledge/embedder/huggingface)
- [Jina](/concepts/knowledge/embedder/jina)
- [Mistral](/concepts/knowledge/embedder/mistral)
- [Ollama](/concepts/knowledge/embedder/ollama)
- [Qdrant FastEmbed](/concepts/knowledge/embedder/qdrant_fastembed)
- [Together](/concepts/knowledge/embedder/together)
- [Voyage AI](/concepts/knowledge/embedder/voyageai)


### Batch Embeddings

Many embedding providers support processing multiple texts in a single API call, known as batch embedding. This approach offers several advantages: it reduces the number of API requests, 
helps avoid rate limits, and significantly improves performance when processing large amounts of text.

To enable batch processing, set the `enable_batch` flag to `True` when configuring your embedder.
The `batch_size` paramater can be used to control the amount of texts sent per batch.

```python
from agno.knowledge.embedder.openai import OpenAIEmbedder

embedder=OpenAIEmbedder(
    id="text-embedding-3-small",
    dimensions=1536,
    enable_batch=True,
    batch_size=100
)
```

The following embedders currently support batching:
- Azure OpenAI
- Cohere
- Fireworks
- Gemini
- Jina
- Mistral
- Nebius
- OpenAI
- Together
- Voyage